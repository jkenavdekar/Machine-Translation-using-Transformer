{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Transformer_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58DnMDpntX_k"
      },
      "source": [
        "# TDDE09 NLP Project - Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75zOl8Iniwyk",
        "outputId": "c463fc36-32d4-4716-b2a4-12c5831268e7"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9VUMWh3jTq_",
        "outputId": "89c96d02-de11-457a-98e7-166bbcfea590"
      },
      "source": [
        "%cd /content/drive/My Drive/NLP_Project/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP_Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL0OjYA1raVk"
      },
      "source": [
        "**Import dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha26xs8_mWVn"
      },
      "source": [
        "import numpy as np\r\n",
        "import math\r\n",
        "import re\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPgoJcbimb8A"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers\r\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzjm6oqgG4nP"
      },
      "source": [
        "**Load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkLrU_TPjcYn"
      },
      "source": [
        "with open(\"europarl-v7.sv-en.en\", mode='r', encoding='utf-8') as f:\r\n",
        "  europarl_en = f.read()\r\n",
        "\r\n",
        "with open(\"europarl-v7.sv-en.sv\", mode='r', encoding='utf-8') as f:\r\n",
        "  europarl_sv = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sc8SaMSGlKMB",
        "outputId": "4010b243-36e5-4698-e7de-aed81c57fc71"
      },
      "source": [
        "europarl_en[:50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the se'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnFSFoGIiOSv"
      },
      "source": [
        "#a.m = a.$$$m = am"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nGBFdmmjjed"
      },
      "source": [
        "### **Data preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B222UAT_kjOx"
      },
      "source": [
        "corpus_en = europarl_en\r\n",
        "#any char following '.' replace it with '.$$$'\r\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\r\n",
        "#now we remove all such instance of '.$$$' from corpus\r\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\r\n",
        "#replace two whitespaces with single whitespace \r\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\r\n",
        "#split each sentence in corpus based on '\\n' new line char\r\n",
        "corpus_en = corpus_en.split('\\n')\r\n",
        "\r\n",
        "corpus_sv = europarl_sv\r\n",
        "corpus_sv = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_sv)\r\n",
        "corpus_sv = re.sub(r\".\\$\\$\\$\", '', corpus_sv)\r\n",
        "corpus_sv = re.sub(r\"  +\", \" \", corpus_sv)\r\n",
        "corpus_sv = corpus_sv.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOVFdOmzmJNZ",
        "outputId": "d8166c82-833c-4090-eda6-5021aeee2f89"
      },
      "source": [
        "corpus_en[:50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Resumption of the session',\n",
              " 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
              " \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\",\n",
              " 'You have requested a debate on this subject in the course of the next few days, during this part-session.',\n",
              " \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\",\n",
              " \"Please rise, then, for this minute' s silence.\",\n",
              " \"(The House rose and observed a minute' s silence)\",\n",
              " 'Madam President, on a point of order.',\n",
              " 'You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.',\n",
              " 'One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.',\n",
              " \"Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\",\n",
              " 'Yes, Mr Evans, I feel an initiative of the type you have just suggested would be entirely appropriate.',\n",
              " 'If the House agrees, I shall do as Mr Evans has suggested.',\n",
              " 'Madam President, on a point of order.',\n",
              " 'I would like your advice about Rule 143 concerning inadmissibility.',\n",
              " 'My question relates to something that will come up on Thursday and which I will then raise again.',\n",
              " 'The Cunha report on multiannual guidance programmes comes before Parliament on Thursday and contains a proposal in paragraph 6 that a form of quota penalties should be introduced for countries which fail to meet their fleet reduction targets annually.',\n",
              " 'It says that this should be done despite the principle of relative stability.',\n",
              " 'I believe that the principle of relative stability is a fundamental legal principle of the common fisheries policy and a proposal to subvert it would be legally inadmissible.',\n",
              " 'I want to know whether one can raise an objection of that kind to what is merely a report, not a legislative proposal, and whether that is something I can competently do on Thursday.',\n",
              " 'That is precisely the time when you may, if you wish, raise this question, ie. on Thursday prior to the start of the presentation of the report.',\n",
              " '',\n",
              " \"Madam President, coinciding with this year' s first part-session of the European Parliament, a date has been set, unfortunately for next Thursday, in Texas in America, for the execution of a young 34 year-old man who has been sentenced to death. We shall call him Mr Hicks.\",\n",
              " 'At the request of a French Member, Mr Zimeray, a petition has already been presented, which many people signed, including myself. However, I would ask you, in accordance with the line which is now constantly followed by the European Parliament and by the whole of the European Community, to make representations, using the weight of your prestigious office and the institution you represent, to the President and to the Governor of Texas, Mr Bush, who has the power to order a stay of execution and to reprieve the condemned person.',\n",
              " 'This is all in accordance with the principles that we have always upheld.',\n",
              " 'Thank you, Mr Segni, I shall do so gladly.',\n",
              " 'Indeed, it is quite in keeping with the positions this House has always adopted.',\n",
              " 'Madam President, I should like to draw your attention to a case in which this Parliament has consistently shown an interest.',\n",
              " 'It is the case of Alexander Nikitin.',\n",
              " 'All of us here are pleased that the courts have acquitted him and made it clear that in Russia, too, access to environmental information is a constitutional right.',\n",
              " 'Now, however, he is to go before the courts once more because the public prosecutor is appealing.',\n",
              " 'We know, and we have stated as much in very many resolutions indeed, including specifically during the last plenary part-session of last year, that this is not solely a legal case and that it is wrong for Alexander Nikitin to be accused of criminal activity and treason because of our involvement as the beneficiaries of his findings.',\n",
              " \"These findings form the basis of the European programmes to protect the Barents Sea, and that is why I would ask you to examine a draft letter setting out the most important facts and to make Parliament's position, as expressed in the resolutions which it has adopted, clear as far as Russia is concerned.\",\n",
              " 'Yes, Mrs Schroedter, I shall be pleased to look into the facts of this case when I have received your letter.',\n",
              " 'Madam President, I would firstly like to compliment you on the fact that you have kept your word and that, during this first part-session of the new year, the number of television channels in our offices has indeed increased considerably.',\n",
              " 'But, Madam President, my personal request has not been met.',\n",
              " 'Although there are now two Finnish channels and one Portuguese one, there is still no Dutch channel, which is what I had requested because Dutch people here like to be able to follow the news too when we are sent to this place of exile every month.',\n",
              " 'I would therefore once more ask you to ensure that we get a Dutch channel as well.',\n",
              " \"Mrs Plooij-van Gorsel, I can tell you that this matter is on the agenda for the Quaestors' meeting on Wednesday.\",\n",
              " 'It will, I hope, be examined in a positive light.',\n",
              " 'Mrs Lynne, you are quite right and I shall check whether this has actually not been done.',\n",
              " 'I shall also refer the matter to the College of Quaestors, and I am certain that they will be keen to ensure that we comply with the regulations we ourselves vote on.',\n",
              " 'Madam President, Mrs Díez González and I had tabled questions on certain opinions of the Vice-President, Mrs de Palacio, which appeared in a Spanish newspaper.',\n",
              " 'The competent services have not included them in the agenda on the grounds that they had been answered in a previous part-session.',\n",
              " 'I would ask that they reconsider, since this is not the case.',\n",
              " \"The questions answered previously referred to Mrs de Palacio' s intervention, on another occasion, and not to these comments which appeared in the ABC newspaper on 18 November.\",\n",
              " 'Mr Berenguer Fuster, we shall check all this.',\n",
              " 'I admit that, at present, the matter seems to be somewhat confused.',\n",
              " 'We shall therefore look into it properly to ensure that everything is as it should be.',\n",
              " 'In any event, this question is not presently included among the requests for topical and urgent debate on Thursday.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "URybEsnfWfuF",
        "outputId": "3ff3d434-711b-46d6-f861-c319de34c687"
      },
      "source": [
        "corpus_en[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Resumption of the session'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDD33woF3Xs_"
      },
      "source": [
        "**Tokenizer will encode each word in sentence with its unique integer value and build a vocab for us. It will also make all sentences to lower case, add spaces before ' . ' and ' , '**. Encoding is fully invertible because all out-of-vocab wordpieces are byte-encoded. Which means unknown word pieces will be encoded one character at a time.\r\n",
        "8192 + 26(all english alphabets) + 2(start, end tokens) = 8221"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl0XtAfqnMfR"
      },
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ZkN2gV4UBr"
      },
      "source": [
        "tokenizer_sv = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_sv, target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAsKXHlcnu4K",
        "outputId": "80b26caa-c490-4d8c-e37f-88f182970f2f"
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 \r\n",
        "VOCAB_SIZE_EN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8221"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uVPeUqDq5mm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd01e332-fcb1-4f26-f7a6-770d8ab4eb19"
      },
      "source": [
        "tokenizer_sv.subwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['att_',\n",
              " ', ',\n",
              " 'och_',\n",
              " 'i_',\n",
              " 'som_',\n",
              " 'för_',\n",
              " 'en_',\n",
              " 'av_',\n",
              " 'det_',\n",
              " 'är_',\n",
              " 'de_',\n",
              " 'till_',\n",
              " 'om_',\n",
              " 'har_',\n",
              " 'på_',\n",
              " 'den_',\n",
              " 'med_',\n",
              " 'inte_',\n",
              " 'vi_',\n",
              " 's_',\n",
              " 'ett_',\n",
              " 't_',\n",
              " 'a_',\n",
              " 'Jag_',\n",
              " 'jag_',\n",
              " 'Det_',\n",
              " 'kommer_',\n",
              " 'kan_',\n",
              " 'måste_',\n",
              " 'detta_',\n",
              " 'r_',\n",
              " 'er_',\n",
              " 'från_',\n",
              " 'Vi_',\n",
              " 'n_',\n",
              " 'vill_',\n",
              " 'skulle_',\n",
              " 'också_',\n",
              " 'så_',\n",
              " 'na_',\n",
              " 'denna_',\n",
              " 'man_',\n",
              " 'EU',\n",
              " 'en',\n",
              " 'mycket_',\n",
              " ' - ',\n",
              " '. ',\n",
              " 'alla_',\n",
              " '! ',\n",
              " 'sig_',\n",
              " 'men_',\n",
              " 'när_',\n",
              " 'Europeiska_',\n",
              " 'vara_',\n",
              " 'eller_',\n",
              " 'talman',\n",
              " 'gäller_',\n",
              " 'Herr_',\n",
              " 'dessa_',\n",
              " 'andra_',\n",
              " 'kommissionen_',\n",
              " ') ',\n",
              " 'även_',\n",
              " 'mot_',\n",
              " 'I_',\n",
              " 'ska_',\n",
              " 'inom_',\n",
              " 'oss_',\n",
              " 'skall_',\n",
              " 'e_',\n",
              " 'finns_',\n",
              " 'utan_',\n",
              " 'under_',\n",
              " 'bara_',\n",
              " 'här_',\n",
              " 'et_',\n",
              " 'bör_',\n",
              " 'er',\n",
              " 'göra_',\n",
              " 'genom_',\n",
              " 'ta_',\n",
              " 'EU_',\n",
              " 'mer_',\n",
              " 'mellan_',\n",
              " 'var_',\n",
              " ': ',\n",
              " 'eftersom_',\n",
              " 'anser_',\n",
              " 'na',\n",
              " 'Detta_',\n",
              " 'nu_',\n",
              " 'kunna_',\n",
              " 'än_',\n",
              " 'vid_',\n",
              " 'nde_',\n",
              " 'vilket_',\n",
              " 'europeiska_',\n",
              " 'därför_',\n",
              " 'få_',\n",
              " 'där_',\n",
              " 'ni_',\n",
              " 'över_',\n",
              " 'ar_',\n",
              " 'ha_',\n",
              " 'fram_',\n",
              " 'Den_',\n",
              " 'allt_',\n",
              " 'våra_',\n",
              " 'nya_',\n",
              " 'För_',\n",
              " 'vad_',\n",
              " 'upp_',\n",
              " 'se_',\n",
              " 'fråga_',\n",
              " 'får_',\n",
              " '- ',\n",
              " 'vår_',\n",
              " 'mig_',\n",
              " 'många_',\n",
              " 'sin_',\n",
              " 'åtgärder_',\n",
              " 'De_',\n",
              " 'rådet_',\n",
              " 'hur_',\n",
              " 'särskilt_',\n",
              " 'första_',\n",
              " 'förslag_',\n",
              " 'parlamentet_',\n",
              " 'ts_',\n",
              " 'as_',\n",
              " ' (',\n",
              " 'd_',\n",
              " 'sina_',\n",
              " 'sätt_',\n",
              " 'något_',\n",
              " 'dem_',\n",
              " 'unionen_',\n",
              " 'redan_',\n",
              " 'år_',\n",
              " 'efter_',\n",
              " 'vilja_',\n",
              " 'ekonomiska_',\n",
              " 'säga_',\n",
              " 'viktigt_',\n",
              " 'et',\n",
              " 'politiska_',\n",
              " 'några_',\n",
              " 'helt_',\n",
              " 'ge_',\n",
              " 'min_',\n",
              " 'kommissionens_',\n",
              " 'Men_',\n",
              " 'deras_',\n",
              " 'frågor_',\n",
              " 'gemensamma_',\n",
              " 'två_',\n",
              " 'bli_',\n",
              " 'olika_',\n",
              " 'vissa_',\n",
              " 'stöd_',\n",
              " 'länder_',\n",
              " 'Om_',\n",
              " 'del_',\n",
              " 'gör_',\n",
              " 'dag_',\n",
              " 'ning_',\n",
              " 'ande_',\n",
              " 'Europa_',\n",
              " 'det',\n",
              " 'frågan_',\n",
              " 'hela_',\n",
              " 'herr_',\n",
              " 'behöver_',\n",
              " 'mänskliga_',\n",
              " 'sitt_',\n",
              " 'medlemsstaterna_',\n",
              " 'någon_',\n",
              " 'betänkande_',\n",
              " 'När_',\n",
              " 'ordförande',\n",
              " 'utskottet_',\n",
              " 'ens_',\n",
              " 'dess_',\n",
              " 'verkligen_',\n",
              " 'dock_',\n",
              " 'rätt_',\n",
              " 'då_',\n",
              " 'Därför_',\n",
              " 'han_',\n",
              " 'tror_',\n",
              " 'stora_',\n",
              " 'mina_',\n",
              " 'hade_',\n",
              " ' – ',\n",
              " 'in_',\n",
              " 'samma_',\n",
              " 'vårt_',\n",
              " 'detta',\n",
              " 'hoppas_',\n",
              " 'fortfarande_',\n",
              " 'ändringsförslag_',\n",
              " 'blir_',\n",
              " 'sedan_',\n",
              " 'ar',\n",
              " 'Europa',\n",
              " 'Fru_',\n",
              " 'betänkandet_',\n",
              " 'stor_',\n",
              " 'rättigheter_',\n",
              " 'vet_',\n",
              " 'nationella_',\n",
              " 'grund_',\n",
              " 'varit_',\n",
              " 'ger_',\n",
              " 'framför_',\n",
              " 'Europaparlamentet_',\n",
              " 'ut_',\n",
              " 'unionens_',\n",
              " 'möjligt_',\n",
              " 'des_',\n",
              " 'sociala_',\n",
              " 'går_',\n",
              " 'ser_',\n",
              " 'problem_',\n",
              " 'En_',\n",
              " 'står_',\n",
              " 'grundläggande_',\n",
              " 'rådets_',\n",
              " 'ingen_',\n",
              " 'innebär_',\n",
              " 'handlar_',\n",
              " 'enligt_',\n",
              " 'arbete_',\n",
              " 'rna_',\n",
              " 'erna_',\n",
              " 'procent_',\n",
              " 'ännu_',\n",
              " 'gå_',\n",
              " 'samt_',\n",
              " 'politik_',\n",
              " 'om',\n",
              " 'exempel_',\n",
              " 'skapa_',\n",
              " 'vilka_',\n",
              " 'människor_',\n",
              " 'borde_',\n",
              " 'Som_',\n",
              " ' ”',\n",
              " 'större_',\n",
              " 'tar_',\n",
              " 'ing_',\n",
              " 'bättre_',\n",
              " 'inför_',\n",
              " 'år',\n",
              " 'tacka_',\n",
              " 'unionen',\n",
              " 'beslut_',\n",
              " 'både_',\n",
              " 'komma_',\n",
              " 'internationella_',\n",
              " 'fall_',\n",
              " 'sätt',\n",
              " 'Kommissionen_',\n",
              " 'bra_',\n",
              " '. - (',\n",
              " 'viktiga_',\n",
              " 'da_',\n",
              " 'liga_',\n",
              " 'just_',\n",
              " 'varje_',\n",
              " 'annat_',\n",
              " 'stödja_',\n",
              " 'ndet_',\n",
              " 'kommissionen',\n",
              " 'senaste_',\n",
              " 'betänkande',\n",
              " 'stöder_',\n",
              " 'ts',\n",
              " 're_',\n",
              " 'herrar',\n",
              " 'medlemsstater_',\n",
              " 'mitt_',\n",
              " 'naturligtvis_',\n",
              " 'ytterligare_',\n",
              " 'nde',\n",
              " 'tid_',\n",
              " 'åt_',\n",
              " 'land_',\n",
              " 'emellertid_',\n",
              " 'samarbete_',\n",
              " 'iska_',\n",
              " 'utveckling_',\n",
              " 'håller_',\n",
              " 'damer_',\n",
              " 'het_',\n",
              " 'viktig_',\n",
              " 'förslaget_',\n",
              " 'punkt_',\n",
              " 'fråga',\n",
              " 'tre_',\n",
              " 're',\n",
              " 'företag_',\n",
              " 'gjort_',\n",
              " 'nas_',\n",
              " 'EN',\n",
              " 'är',\n",
              " 'roll_',\n",
              " 'gång_',\n",
              " '\\xa0\\xa0 – ',\n",
              " 'as',\n",
              " 'mål_',\n",
              " 'fortsätta_',\n",
              " 'ning',\n",
              " 'an_',\n",
              " 'själva_',\n",
              " 'ade_',\n",
              " 'parlamentets_',\n",
              " 'situationen_',\n",
              " 'europeisk_',\n",
              " 'it_',\n",
              " 'de',\n",
              " 'miljoner_',\n",
              " 'endast_',\n",
              " 'till',\n",
              " 'inre_',\n",
              " 'kanske_',\n",
              " 'dag',\n",
              " 'frågor',\n",
              " 'artikel_',\n",
              " 'avtal_',\n",
              " 'lägga_',\n",
              " 'flera_',\n",
              " 'faktiskt_',\n",
              " 'rättsliga_',\n",
              " 'vilken_',\n",
              " '\\xa0',\n",
              " 'bland_',\n",
              " 'ligt_',\n",
              " 'tydligt_',\n",
              " 'visar_',\n",
              " 'hänsyn_',\n",
              " 'samtidigt_',\n",
              " 'o_',\n",
              " 'nämligen_',\n",
              " 'alltid_',\n",
              " 'hans_',\n",
              " 'Låt_',\n",
              " 'Förenta_',\n",
              " 'mindre_',\n",
              " 'rättigheter',\n",
              " 'arna_',\n",
              " 'å_',\n",
              " 'enda_',\n",
              " 'tidigare_',\n",
              " 'antal_',\n",
              " 'system_',\n",
              " 'an',\n",
              " 'nuvarande_',\n",
              " 'möjlighet_',\n",
              " 'medborgare_',\n",
              " 'ligger_',\n",
              " 'marknaden_',\n",
              " 'ns_',\n",
              " 'nödvändigt_',\n",
              " 'områden_',\n",
              " 'ny_',\n",
              " 'dem',\n",
              " 'området_',\n",
              " 'förbättra_',\n",
              " 'het',\n",
              " 'Vad_',\n",
              " 'emot_',\n",
              " 'länderna_',\n",
              " 'miljö',\n",
              " 'ramen_',\n",
              " 'främja_',\n",
              " 'direktiv_',\n",
              " 'röstade_',\n",
              " 'hos_',\n",
              " 'ra_',\n",
              " 'Denna_',\n",
              " 'för',\n",
              " 'Med_',\n",
              " 'era_',\n",
              " 'tredje_',\n",
              " 'gemensam_',\n",
              " 'den',\n",
              " 'rådet',\n",
              " 'ekonomisk_',\n",
              " 'utgör_',\n",
              " 'stort_',\n",
              " 'små_',\n",
              " 'förslag',\n",
              " 'lika_',\n",
              " 'öka_',\n",
              " 'väl_',\n",
              " 'tanke_',\n",
              " 'På_',\n",
              " 'information_',\n",
              " 'energi',\n",
              " 'medel_',\n",
              " 'Att_',\n",
              " 'på',\n",
              " 'välkomnar_',\n",
              " 'längre_',\n",
              " 'stöd',\n",
              " 'al',\n",
              " 'genomföra_',\n",
              " 'g_',\n",
              " 'kvinnor_',\n",
              " 'sidan_',\n",
              " 'tillsammans_',\n",
              " 'Man_',\n",
              " 'länder',\n",
              " 'tycker_',\n",
              " 'Ett_',\n",
              " 'här',\n",
              " 'are_',\n",
              " 'l_',\n",
              " 'trots_',\n",
              " 'samband_',\n",
              " 'or',\n",
              " 'använda_',\n",
              " 'vi',\n",
              " 'direktivet_',\n",
              " 'säger_',\n",
              " 'at_',\n",
              " 'land',\n",
              " 'ningen_',\n",
              " 'ofta_',\n",
              " 'mest_',\n",
              " 'parlamentet',\n",
              " 'debatt_',\n",
              " 'alltså_',\n",
              " 'medlemsstaterna',\n",
              " 'lig_',\n",
              " 'garantera_',\n",
              " 'föredraganden_',\n",
              " 'fått_',\n",
              " '; ',\n",
              " 'talar_',\n",
              " 'närvarande_',\n",
              " 'tt_',\n",
              " 'personer_',\n",
              " 'kräver_',\n",
              " 'skriftlig',\n",
              " 'ka_',\n",
              " 'grupp_',\n",
              " 'politisk_',\n",
              " 'nivå',\n",
              " 'Även_',\n",
              " 'Europaparlamentets_',\n",
              " '” ',\n",
              " 'Dessa_',\n",
              " 'program_',\n",
              " 'uppnå_',\n",
              " 'klart_',\n",
              " 'åtgärder',\n",
              " 'ande',\n",
              " 'annan_',\n",
              " 'minska_',\n",
              " 'nivå_',\n",
              " 'största_',\n",
              " 'krävs_',\n",
              " 'debatten_',\n",
              " 'val',\n",
              " 'k_',\n",
              " 'område_',\n",
              " 'es_',\n",
              " '1_',\n",
              " 'stället_',\n",
              " 'sådana_',\n",
              " 'or_',\n",
              " 'rör_',\n",
              " 'kommissionär',\n",
              " 'rösta_',\n",
              " 'utvecklingen_',\n",
              " 'delar_',\n",
              " 'bidra_',\n",
              " 'm_',\n",
              " 'tala_',\n",
              " 'med',\n",
              " 'kunde_',\n",
              " 'gruppen_',\n",
              " 'ni',\n",
              " 'Ni_',\n",
              " 'ansvar_',\n",
              " 'tas_',\n",
              " 'offentliga_',\n",
              " 'betänkandet',\n",
              " 'egna_',\n",
              " 'synnerhet_',\n",
              " 'DE',\n",
              " 'ningar_',\n",
              " 'oss',\n",
              " 'steg_',\n",
              " 'framsteg_',\n",
              " 'ur_',\n",
              " 'rättigheterna_',\n",
              " 'sådan_',\n",
              " 'hjälpa_',\n",
              " 'problem',\n",
              " 'rum_',\n",
              " 'allmänna_',\n",
              " 'ordförandeskapet_',\n",
              " 'initiativ_',\n",
              " 'skydda_',\n",
              " 'först_',\n",
              " 'behovet_',\n",
              " 'finansiella_',\n",
              " 'ri',\n",
              " 'arbete',\n",
              " 'av',\n",
              " 'nings',\n",
              " 'budget',\n",
              " 'området',\n",
              " 'strategi_',\n",
              " 'utveckling',\n",
              " 'fru_',\n",
              " 'medlemsstaternas_',\n",
              " 'hjälp_',\n",
              " 'tillgång_',\n",
              " 'nå_',\n",
              " 'leda_',\n",
              " 'heller_',\n",
              " 'kommissionsledamot',\n",
              " 'ord_',\n",
              " 'mål',\n",
              " 'st_',\n",
              " 'iga_',\n",
              " 'ordförande_',\n",
              " 'upp',\n",
              " 'känner_',\n",
              " 'visa_',\n",
              " 'dessutom_',\n",
              " 'ut',\n",
              " 'medlemsstater',\n",
              " 'ing',\n",
              " 'lösa_',\n",
              " 'krav_',\n",
              " 'hålla_',\n",
              " 'st',\n",
              " 'ert_',\n",
              " 'medborgarna_',\n",
              " 'börja_',\n",
              " 'ns',\n",
              " 'skydd_',\n",
              " 'säkerhet_',\n",
              " 'nästa_',\n",
              " 'politik',\n",
              " 'båda_',\n",
              " 'bästa_',\n",
              " 'avgörande_',\n",
              " 'arbeta_',\n",
              " 'viktigaste_',\n",
              " 'nu',\n",
              " 'tt',\n",
              " 'lagstiftning_',\n",
              " 'liksom_',\n",
              " 'kl',\n",
              " 'ra',\n",
              " 'social_',\n",
              " 'euro_',\n",
              " 'Under_',\n",
              " 'möjligheter_',\n",
              " 'ledamöter_',\n",
              " 'Och_',\n",
              " 'nytt_',\n",
              " 'Dessutom_',\n",
              " 'ändå_',\n",
              " 'all_',\n",
              " 'svar_',\n",
              " 'att',\n",
              " 'regeringen_',\n",
              " 'fiske',\n",
              " 'enlighet_',\n",
              " 'fler_',\n",
              " 'ma_',\n",
              " 'es',\n",
              " 'ro',\n",
              " 'avtalet_',\n",
              " 'införa_',\n",
              " 'ju_',\n",
              " 'bekämpa_',\n",
              " '5_',\n",
              " 'hög_',\n",
              " 'omfattande_',\n",
              " 'lagt_',\n",
              " 'världen',\n",
              " 'marknaden',\n",
              " 'uppmanar_',\n",
              " 'am',\n",
              " 'Europas_',\n",
              " 'tjänster_',\n",
              " 'kommande_',\n",
              " 'ber',\n",
              " 'on_',\n",
              " 'sk_',\n",
              " 'arbets',\n",
              " 'världen_',\n",
              " 'före_',\n",
              " 'politiken_',\n",
              " 'innan_',\n",
              " 'sagt_',\n",
              " 'demokratiska_',\n",
              " 'gemenskapens_',\n",
              " 'lag',\n",
              " 'företag',\n",
              " 'framtida_',\n",
              " 'ng',\n",
              " 'faktum_',\n",
              " 'dra_',\n",
              " 'lo',\n",
              " 'situation_',\n",
              " 'systemet_',\n",
              " 'frågan',\n",
              " 'sett_',\n",
              " 'säkerhets',\n",
              " 'tagit_',\n",
              " 'landet_',\n",
              " 'gen',\n",
              " 'snabbt_',\n",
              " 'konkurrens',\n",
              " 'te_',\n",
              " 'område',\n",
              " 'egen_',\n",
              " 'staterna_',\n",
              " 'lösning_',\n",
              " 'förra_',\n",
              " 'Enligt_',\n",
              " 'inte',\n",
              " 'icke',\n",
              " 'el',\n",
              " 'regler_',\n",
              " 'resolution_',\n",
              " 'instrument_',\n",
              " 'FN',\n",
              " 'väl',\n",
              " 'medelstora_',\n",
              " 'tiden_',\n",
              " 'ja',\n",
              " 'le',\n",
              " 'under',\n",
              " 'försöka_',\n",
              " 'fram',\n",
              " 'do',\n",
              " 'slut',\n",
              " 'bestämmelser_',\n",
              " 'projekt_',\n",
              " 'gräns',\n",
              " 'främst_',\n",
              " 'erna',\n",
              " 'ram',\n",
              " 'ga_',\n",
              " 'resultat_',\n",
              " 'andra',\n",
              " 'sade_',\n",
              " 'rna',\n",
              " 'blivit_',\n",
              " 'klar',\n",
              " 'kolleger_',\n",
              " 'behov_',\n",
              " '2_',\n",
              " 'ed',\n",
              " 'syfte_',\n",
              " 'ad_',\n",
              " 'verkar_',\n",
              " 'effektivt_',\n",
              " 'form_',\n",
              " 'tillbaka_',\n",
              " 'stärka_',\n",
              " 'uppgifter_',\n",
              " 'bet',\n",
              " 'produkter_',\n",
              " 'vidta_',\n",
              " 'innehåller_',\n",
              " 'li',\n",
              " '000_',\n",
              " 'leder_',\n",
              " 'ha',\n",
              " 'procent',\n",
              " 'll',\n",
              " 'problemet_',\n",
              " 'syftar_',\n",
              " 'framtiden',\n",
              " 'utveckla_',\n",
              " 'go',\n",
              " 'särskilda_',\n",
              " 'områden',\n",
              " 'och',\n",
              " 'gi',\n",
              " 'tillräckligt_',\n",
              " 'sig',\n",
              " 'ag',\n",
              " 'da',\n",
              " 'itu_',\n",
              " 'gruppen',\n",
              " 'forskning_',\n",
              " 'alltför_',\n",
              " 'in',\n",
              " 'alla',\n",
              " 'ståndpunkt_',\n",
              " 'minst_',\n",
              " 'nödvändiga_',\n",
              " 'ligen_',\n",
              " '\" ',\n",
              " 'visat_',\n",
              " 'försöker_',\n",
              " 'genomförandet_',\n",
              " 'hälso',\n",
              " 'gjorde_',\n",
              " 'la_',\n",
              " 'grund',\n",
              " 'finnas_',\n",
              " 'man',\n",
              " 'ter',\n",
              " 'direkt_',\n",
              " 'on',\n",
              " 'kampen_',\n",
              " 'såsom_',\n",
              " 'Till_',\n",
              " 'enbart_',\n",
              " 'fortsätter_',\n",
              " 'sådant_',\n",
              " 'inget_',\n",
              " 'fullt_',\n",
              " 'tid',\n",
              " 'ta',\n",
              " 'ter_',\n",
              " 'vare_',\n",
              " 'Europaparlamentet',\n",
              " 'tänker_',\n",
              " '3_',\n",
              " 'folk',\n",
              " 'villkor_',\n",
              " 'har',\n",
              " 'långt_',\n",
              " 'möjligt',\n",
              " 'ber_',\n",
              " 'aldrig_',\n",
              " 'hon_',\n",
              " 'framtiden_',\n",
              " 'anta_',\n",
              " 'bidrag_',\n",
              " 'fick_',\n",
              " 'fullständigt_',\n",
              " 'fe',\n",
              " '0_',\n",
              " 'gratulera_',\n",
              " 'betydelse_',\n",
              " 'Hur_',\n",
              " 'pe',\n",
              " 'rad_',\n",
              " 'myndigheterna_',\n",
              " 'absolut_',\n",
              " 'vidare_',\n",
              " 'där',\n",
              " 'snart_',\n",
              " 'över',\n",
              " 'kolleger',\n",
              " 'saker_',\n",
              " 'avtal',\n",
              " 'ena_',\n",
              " 'nyligen_',\n",
              " 'skäl_',\n",
              " 'enkelt_',\n",
              " 'därmed_',\n",
              " 'sista_',\n",
              " 'barn_',\n",
              " 'fast_',\n",
              " 'miss',\n",
              " 'EG',\n",
              " 'Nu_',\n",
              " 'haft_',\n",
              " 'barn',\n",
              " 'föra_',\n",
              " 'rätt',\n",
              " 'resurser_',\n",
              " 'ad',\n",
              " 'goda_',\n",
              " 'v_',\n",
              " 'föreslår_',\n",
              " 'politiskt_',\n",
              " 'utanför_',\n",
              " 'la',\n",
              " 'rätta_',\n",
              " 'inga_',\n",
              " 'världs',\n",
              " 'y_',\n",
              " 'varför_',\n",
              " 'viss_',\n",
              " 'behövs_',\n",
              " 'internationell_',\n",
              " 'betona_',\n",
              " 'som',\n",
              " 'di',\n",
              " 'vore_',\n",
              " 'åtminstone_',\n",
              " 'ord',\n",
              " 'ot',\n",
              " 'kort_',\n",
              " 'andet_',\n",
              " 'lång_',\n",
              " 'mi',\n",
              " 'um',\n",
              " 'hand_',\n",
              " 'hållbar_',\n",
              " 'Så_',\n",
              " 'tra',\n",
              " 'ul',\n",
              " 'ud',\n",
              " 'länge_',\n",
              " 'flyg',\n",
              " 'nära_',\n",
              " 'ko',\n",
              " 'Samtidigt_',\n",
              " 'vis_',\n",
              " 'Ryssland_',\n",
              " 'fördraget_',\n",
              " 'arbetet_',\n",
              " 'ten_',\n",
              " '4_',\n",
              " 'ma',\n",
              " 'euro',\n",
              " 'transport',\n",
              " 'ekonomiskt_',\n",
              " 'svårt_',\n",
              " 'grad_',\n",
              " '00',\n",
              " 'väg_',\n",
              " 'följd_',\n",
              " 'kommit_',\n",
              " 'pengar_',\n",
              " 'göra',\n",
              " 'at',\n",
              " 'arbetar_',\n",
              " 'högre_',\n",
              " 'fallet_',\n",
              " 'tex',\n",
              " 'medborgare',\n",
              " 'tekniska_',\n",
              " 'intresse_',\n",
              " 'Eftersom_',\n",
              " 'ställa_',\n",
              " 'rättigheterna',\n",
              " 'kollega_',\n",
              " 'stor',\n",
              " 'avslutad',\n",
              " 'vatten',\n",
              " 'sam',\n",
              " 'verksamhet_',\n",
              " 'programmet_',\n",
              " 'diskutera_',\n",
              " 'mig',\n",
              " 'fria_',\n",
              " 'Turkiet_',\n",
              " 'rätten_',\n",
              " 'Tack_',\n",
              " 'För',\n",
              " 'så',\n",
              " 'överens_',\n",
              " 'are',\n",
              " '\\xa0\\xa0 . – ',\n",
              " 'kammaren_',\n",
              " 'va_',\n",
              " 'såväl_',\n",
              " 'principen_',\n",
              " 'it',\n",
              " 'bort_',\n",
              " 'Le',\n",
              " 'vin',\n",
              " 'språk',\n",
              " 'ton',\n",
              " 'låta_',\n",
              " 'kontroll',\n",
              " 'svar',\n",
              " 'vars_',\n",
              " 'insatser_',\n",
              " 'precis_',\n",
              " 'handel_',\n",
              " 'globala_',\n",
              " 'regionala_',\n",
              " 'ki',\n",
              " 'Slutligen_',\n",
              " 'fall',\n",
              " 'pp',\n",
              " 'sta',\n",
              " 'é',\n",
              " 'mo',\n",
              " 'be_',\n",
              " 'förhandlingarna_',\n",
              " 'sak',\n",
              " 'undvika_',\n",
              " 'ck',\n",
              " 'samarbetet_',\n",
              " 'handels',\n",
              " 'Nästa_',\n",
              " 'beslut',\n",
              " 'intresse',\n",
              " 'enskilda_',\n",
              " 'system',\n",
              " 'antalet_',\n",
              " 'kontroll_',\n",
              " 'tyvärr_',\n",
              " 'ng_',\n",
              " 'Ma',\n",
              " 'dagens_',\n",
              " '”.',\n",
              " 'is',\n",
              " 'processen_',\n",
              " 'Genom_',\n",
              " 'början_',\n",
              " 'nästan_',\n",
              " 'följa_',\n",
              " 'utbildning_',\n",
              " 'ske_',\n",
              " 'åter',\n",
              " 'avseende_',\n",
              " 'bi',\n",
              " 'beroende_',\n",
              " 'själv_',\n",
              " 'sför',\n",
              " 'ol',\n",
              " 'ör',\n",
              " 'intressen',\n",
              " 'oberoende_',\n",
              " 'Trots_',\n",
              " 'ings',\n",
              " 'Parlamentet_',\n",
              " 'stats',\n",
              " 'tids',\n",
              " 'slutet_',\n",
              " 'fem_',\n",
              " 'nationell_',\n",
              " 'krisen_',\n",
              " 'ir',\n",
              " 'su',\n",
              " 'kommissionsledamoten',\n",
              " 'tro',\n",
              " 'sker_',\n",
              " 'heten_',\n",
              " 'efter',\n",
              " '2000',\n",
              " 'gemensamt_',\n",
              " 'Ka',\n",
              " 'el_',\n",
              " 'konkreta_',\n",
              " 'Alla_',\n",
              " 'betydande_',\n",
              " 'Vi',\n",
              " 'allvarliga_',\n",
              " 'äga_',\n",
              " 'z_',\n",
              " 'förhindra_',\n",
              " 'ned',\n",
              " 'ka',\n",
              " 'exempelvis_',\n",
              " '6_',\n",
              " 'fri',\n",
              " 'ur',\n",
              " 'bygga_',\n",
              " 'fyra_',\n",
              " 'Ta',\n",
              " 'positiva_',\n",
              " 'jordbruks',\n",
              " 'kunnat_',\n",
              " 'to',\n",
              " 'sätta_',\n",
              " 'budget_',\n",
              " 'rar_',\n",
              " 'program',\n",
              " 'tre',\n",
              " 'dvs',\n",
              " 'europeiskt_',\n",
              " 'gemenskaps',\n",
              " 'själv',\n",
              " 'bakom_',\n",
              " 'ci',\n",
              " 'ver',\n",
              " 'du',\n",
              " 'sektorn_',\n",
              " 'tack_',\n",
              " 'menar_',\n",
              " 'demokrati',\n",
              " 'ål',\n",
              " 'medan_',\n",
              " 'var',\n",
              " 'dialog_',\n",
              " 'snarare_',\n",
              " 'helst_',\n",
              " 'åt',\n",
              " 'lokala_',\n",
              " 'hjälp',\n",
              " 'berörda_',\n",
              " 'Sa',\n",
              " 'kon',\n",
              " ' \" ',\n",
              " 'bil',\n",
              " 'Min_',\n",
              " 'eri',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlWiT8KlcB3r",
        "outputId": "a1eefaa7-3597-4123-a859-d7c652f2758f"
      },
      "source": [
        "VOCAB_SIZE_SV = tokenizer_sv.vocab_size + 2\r\n",
        "VOCAB_SIZE_SV"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8234"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DHC0Cz_qmdy"
      },
      "source": [
        "**Pad the 'start' and 'end' token to all sentences in the corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sro1ih4ZuBh-"
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1] for sentence in corpus_en]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq8lN013cVKC"
      },
      "source": [
        "outputs = [[VOCAB_SIZE_SV-2] + tokenizer_sv.encode(sentence) + [VOCAB_SIZE_SV-1] for sentence in corpus_sv]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GCBnbnGvJF0",
        "outputId": "5d3ebb52-a0c2-4adb-cffb-80a8f3c86dec"
      },
      "source": [
        "inputs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8219, 2561, 1009, 2044, 3, 1, 2573, 8220]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QhSZMz_EvS6Z",
        "outputId": "79b6f8b5-7abe-4f61-bdc7-e2dd1fd7a94e"
      },
      "source": [
        "corpus_en[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Resumption of the session'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DsMEoZacn4g",
        "outputId": "adb4bddf-72a8-4bd9-98c7-4accb5b1087d"
      },
      "source": [
        "outputs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8232, 3338, 78, 7341, 5898, 8, 5836, 44, 8233]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dENk6p3Pput7"
      },
      "source": [
        "**Get sentences having Max length of 20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGnqe-Ql6e5N"
      },
      "source": [
        "MAX_LENGTH = 20\r\n",
        "\r\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs) if len(sent) > MAX_LENGTH]\r\n",
        "\r\n",
        "#delete sentences from inputs(source) that exceed max len of 20, correspondingly also delete sentences in outputs(target).\r\n",
        "for idx in reversed(idx_to_remove):\r\n",
        "  del inputs[idx]\r\n",
        "  del outputs[idx]\r\n",
        "\r\n",
        "#we do the same thing for outputs(target) ie: find sent that exceed max len of 20 in target outputs and del from both outputs \r\n",
        "#and inputs\r\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs) if len(sent) > MAX_LENGTH]\r\n",
        "for idx in reversed(idx_to_remove):\r\n",
        "  del inputs[idx]\r\n",
        "  del outputs[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOftLKzA7Mxi"
      },
      "source": [
        "**Pad value of 0 for sentences less than its max length**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyQuEzzd7h1G"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, value=0, padding='post', maxlen=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RChAnCJU7sn4",
        "outputId": "f0efb9de-b60f-4806-fdd9-df84d4449e79"
      },
      "source": [
        "inputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(444842, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRSGbGfDeMCj"
      },
      "source": [
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs, value=0, padding='post', maxlen=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoKGYd2Aen2Y",
        "outputId": "d3812d1d-308d-4dc9-9659-d74b3da4650f"
      },
      "source": [
        "outputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(444842, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h54QSTWNna9",
        "outputId": "c1535e52-365a-4eb5-c48b-79c2926536a3"
      },
      "source": [
        "valid_src = inputs[-1000:]\n",
        "valid_ref = outputs[-1000:]\n",
        "valid_src.shape, valid_ref.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 20), (1000, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdZXp64CcgP9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSw6oKpOOAaz",
        "outputId": "a91f2612-b553-4bd3-9e29-1d3296b11807"
      },
      "source": [
        "inputs = inputs[:-1000]\r\n",
        "outputs = outputs[:-1000]\r\n",
        "inputs.shape, outputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((443842, 20), (443842, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG1t1E9jeu1R"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "BUFFER_SIZE = 20000\r\n",
        "\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\r\n",
        "\r\n",
        "#to help increase speed during training - store it in cache\r\n",
        "dataset = dataset.cache()\r\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n",
        "#to help access to data faster - further improving speed (Note: it has no effect on accuracy)\r\n",
        "#This transformation basically uses a background thread and an internal buffer to prefetch elements \r\n",
        "#from the input dataset ahead of the time they are requested.\r\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YLnyluZsCmt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0833ae6-49fc-4d38-af0b-36a52ecf1e93"
      },
      "source": [
        "next(iter(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 20), dtype=int32, numpy=\n",
              " array([[8219,   11, 5611, ...,    0,    0,    0],\n",
              "        [8219,   25,  554, ...,    0,    0,    0],\n",
              "        [8219,   29,   43, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8219,   25,  516, ...,    0,    0,    0],\n",
              "        [8219, 8220,    0, ...,    0,    0,    0],\n",
              "        [8219,   11,  343, ...,    0,    0,    0]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(64, 20), dtype=int32, numpy=\n",
              " array([[8232,   24, 2491, ...,  823, 8022, 8233],\n",
              "        [8232, 2231,   10, ...,    0,    0,    0],\n",
              "        [8232,   58,   56, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8232, 7834,  182, ...,    0,    0,    0],\n",
              "        [8232,   58,   56, ...,    0,    0,    0],\n",
              "        [8232,   24,  404, ...,    0,    0,    0]], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVF5o4yBYvQ3"
      },
      "source": [
        "## Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksBkAebPUpDe"
      },
      "source": [
        "**Embeddings represent a word in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the similarity of their meaning and also their position in the sentence. We represent this using the following formula:**<br><br>\r\n",
        "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\r\n",
        "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$\r\n",
        "<br>\r\n",
        "**For each each dimension of an embedding which is represented as 'i' we will get cosine function w.r.t the position in the sequence.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PxIclTyBnP4"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super(PositionalEncoding, self).__init__()\r\n",
        "    \r\n",
        "    def get_angles(self, pos, i, d_model):\r\n",
        "        #pos -> [seqlen, 1] , i -> [1, d_model] , d_model -> embedding dimension size\r\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\r\n",
        "        return pos * angles\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        #get the first dimension of the input tensor (seq length)\r\n",
        "        seq_length = inputs.shape.as_list()[-2]\r\n",
        "        #get the second dimension of the input tensor (embedding dim)\r\n",
        "        d_model = inputs.shape.as_list()[-1]\r\n",
        "        #send list of positions from 0 to seq length with an additional axis [seq, 1], send list of dimensions [1, dim]\r\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\r\n",
        "        #all angle values 0 to last with a step of 2 (to access even part)\r\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\r\n",
        "        #1:last:2 with step of 2 to access the odd part\r\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\r\n",
        "        # add extra dim [1, seq, d_model] to accomodate batch size [batch, seq, d_model]\r\n",
        "        pos_encoding = angles[np.newaxis, ...]\r\n",
        "        #concat (add the embedding input to pos encoding (convert to tensor))\r\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frYysB5PZRqR"
      },
      "source": [
        "## Scaled Dot Product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd01IcpD6ICa"
      },
      "source": [
        "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvT5ErCbp84U"
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\r\n",
        "    #Q, K and V  size -> [batch, nb.proj, seq, nb.proj.dim]\r\n",
        "    #matrix multiply query with the transpose of key matrix => [batch, nb.proj, 20, 20]\r\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\r\n",
        "\r\n",
        "    #get the keys dimension size and type caste it to float\r\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\r\n",
        "\r\n",
        "    #scale the product by dimension size -> so as to get consistent variance regardless the value of dim\r\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\r\n",
        "    \r\n",
        "    #if there is a valid mask!\r\n",
        "    if mask is not None:\r\n",
        "        #padded values get multiplied by a very large negative number ~ close to negative infinity\r\n",
        "        #this makes sure that softmax applied on the padded values go to zeroes. \r\n",
        "        scaled_product += (mask * -1e9)\r\n",
        "    \r\n",
        "    #finally we apply softmax along last dimension such that prob of seq sum up to 1, \r\n",
        "    #the result is multiplied with values matrix.\r\n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)  # [batch, nb.proj, seq, nb.proj.dim]\r\n",
        "    \r\n",
        "    return attention, tf.nn.softmax(scaled_product, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpPHdDlOkMbe",
        "outputId": "d29259e4-00d4-4157-b386-f6c957302255"
      },
      "source": [
        "x = tf.random.uniform((64, 8, 20, 64))\r\n",
        "product, _ = scaled_dot_product_attention(x, x, x, mask=None)\r\n",
        "product.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 8, 20, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFdy_i_kvXc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abaf5e6a-8f02-48af-b9c6-552f6ac1b0a5"
      },
      "source": [
        "[2, 4, 6, 0, 0 ,0 ] , [0, 0, 0 , 1, 1, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2, 4, 6, 0, 0, 0], [0, 0, 0, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFV3S5GsZwX4"
      },
      "source": [
        "## Multi Head Attention "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w_ynABn_G2j"
      },
      "source": [
        "<center><img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"300\" alt=\"multi-head attention\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsbPguu13Ha2"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\r\n",
        "    \r\n",
        "    def __init__(self, nb_proj):\r\n",
        "        #call the base super class\r\n",
        "        super(MultiHeadAttention, self).__init__()\r\n",
        "        #initialize the no. of projections\r\n",
        "        self.nb_proj = nb_proj\r\n",
        "        \r\n",
        "    def build(self, input_shape):\r\n",
        "        #get the dimension (d_model)\r\n",
        "        self.d_model = input_shape[-1]\r\n",
        "\r\n",
        "        #we check if the d_model dimension is divisible by no. of proj\r\n",
        "        assert self.d_model % self.nb_proj == 0\r\n",
        "        \r\n",
        "        #divide and get only the integer part of the fraction\r\n",
        "        self.d_proj = self.d_model // self.nb_proj\r\n",
        "        \r\n",
        "        #define dense layers having d_model as hidden units for Q, K and V\r\n",
        "        self.query_lin = layers.Dense(units=self.d_model)\r\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\r\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\r\n",
        "        \r\n",
        "        #Final output linear layer\r\n",
        "        self.final_lin = layers.Dense(units=self.d_model)\r\n",
        "        \r\n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\r\n",
        "        #we define the split shape we want for our output tensor here [Batch, seq, nb_proj, dim_proj]\r\n",
        "        shape = (batch_size, -1, self.nb_proj, self.d_proj)\r\n",
        "\r\n",
        "        #Now we reshape the inputs into the above defined shape\r\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\r\n",
        "\r\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\r\n",
        "    \r\n",
        "    def call(self, queries, keys, values, mask):\r\n",
        "        #get the bactch size\r\n",
        "        batch_size = tf.shape(queries)[0]\r\n",
        "        \r\n",
        "        #apply the layers onto Q, K and V\r\n",
        "        queries = self.query_lin(queries)\r\n",
        "        keys = self.key_lin(keys)\r\n",
        "        values = self.value_lin(values)\r\n",
        "        \r\n",
        "        #we get the splitted projections for Q, K and V respectively\r\n",
        "        queries = self.split_proj(queries, batch_size)\r\n",
        "        keys = self.split_proj(keys, batch_size)\r\n",
        "        values = self.split_proj(values, batch_size)\r\n",
        "        \r\n",
        "        #get the attention weights\r\n",
        "        attention, weights = scaled_dot_product_attention(queries, keys, values, mask)\r\n",
        "        \r\n",
        "        #permute and get back original tensor shape of [batch, seq, nb.proj, dim_proj]\r\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\r\n",
        "        \r\n",
        "        #merge and reshape back into [Batch, seq, nb.proj * dim_proj] = [Batch, seq, d_model]\r\n",
        "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\r\n",
        " \r\n",
        "        \r\n",
        "        outputs = self.final_lin(concat_attention)  #[Batch, Seq, d_model]\r\n",
        "\r\n",
        "        return outputs, weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDVsi1bOGZDu"
      },
      "source": [
        "temp_mha = MultiHeadAttention(8)\r\n",
        "y = tf.random.uniform((64, 20, 512))  # (batch_size, seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxrJ5t60G_fj",
        "outputId": "9b7cb066-4c87-430f-f8a2-3b1e30fd621c"
      },
      "source": [
        "out, w = temp_mha(y, y, y, mask=None)\r\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 20, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbazonNVZ4C-"
      },
      "source": [
        "## Encoder-Decoder Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WkyTDfqrz6o"
      },
      "source": [
        "<center><img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"400\" alt=\"transformer\">\r\n",
        "<br><center>Transformer Model\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EGqYzIoKUnV"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\r\n",
        "    \r\n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\r\n",
        "        super(EncoderLayer, self).__init__()\r\n",
        "        self.FFN_units = FFN_units\r\n",
        "        self.nb_proj = nb_proj\r\n",
        "        self.dropout_rate = dropout_rate\r\n",
        "    \r\n",
        "    def build(self, input_shape):\r\n",
        "        self.d_model = input_shape[-1]\r\n",
        "        \r\n",
        "        #Define MHA for the encoder\r\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\r\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\r\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        \r\n",
        "        #Define the FFN layer!\r\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\r\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\r\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\r\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        \r\n",
        "    def call(self, inputs, mask, training, index, flag):\r\n",
        "\r\n",
        "        #test case: reordering of sublayers in encoder\r\n",
        "        if index == 0 and flag:\r\n",
        "            attention, _ = self.multi_head_attention(inputs, inputs, inputs, mask)\r\n",
        "            attention = self.dropout_1(attention, training=training)\r\n",
        "            attention = self.norm_1(attention + inputs)\r\n",
        "\r\n",
        "            inputs = attention + inputs\r\n",
        "\r\n",
        "            attention, _ = self.multi_head_attention(inputs, inputs, inputs, mask)\r\n",
        "            attention = self.dropout_1(attention, training=training)\r\n",
        "            attention = self.norm_1(attention + inputs)\r\n",
        "\r\n",
        "            return attention\r\n",
        "        \r\n",
        "        if index == 3 and flag:\r\n",
        "            outputs = self.dense_1(inputs)\r\n",
        "            outputs = self.dense_2(outputs)\r\n",
        "            outputs = self.dropout_2(outputs, training=training)\r\n",
        "            outputs = self.norm_2(outputs + inputs)\r\n",
        "\r\n",
        "            outputs = self.dense_1(inputs)\r\n",
        "            outputs = self.dense_2(outputs)\r\n",
        "            outputs = self.dropout_2(outputs, training=training)\r\n",
        "            outputs = self.norm_2(outputs + inputs)\r\n",
        "\r\n",
        "            return outputs\r\n",
        "\r\n",
        "\r\n",
        "        #call MHA here with the query, key and value == input\r\n",
        "        attention, _ = self.multi_head_attention(inputs, inputs, inputs, mask)\r\n",
        "        #apply dropout for regularization\r\n",
        "        attention = self.dropout_1(attention, training=training)\r\n",
        "        #perfrom add and normalization\r\n",
        "        attention = self.norm_1(attention + inputs)\r\n",
        "        \r\n",
        "        outputs = self.dense_1(attention)\r\n",
        "        outputs = self.dense_2(outputs)\r\n",
        "        outputs = self.dropout_2(outputs, training=training)\r\n",
        "        outputs = self.norm_2(outputs + attention)\r\n",
        "        \r\n",
        "        return outputs # [Batch, Seq, D_model]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdQnf8aBBTQG",
        "outputId": "19b137e7-e144-4f08-dd6b-7f67cd821b6e"
      },
      "source": [
        "EL = EncoderLayer(1024, 8, 0.1)\r\n",
        "x = tf.random.uniform((64, 20, 512))\r\n",
        "EL(x, None, False, 4, True).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 20, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOnGMNsvZp_2"
      },
      "source": [
        "class Encoder(layers.Layer):\r\n",
        "    \r\n",
        "    def __init__(self, nb_layers, FFN_units, nb_proj, dropout_rate, vocab_size, d_model, name=\"encoder\"):\r\n",
        "        super(Encoder, self).__init__(name=name)\r\n",
        "        self.nb_layers = nb_layers\r\n",
        "        self.d_model = d_model\r\n",
        "        \r\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\r\n",
        "        self.pos_encoding = PositionalEncoding()\r\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\r\n",
        "        #create 'N' Encodinglayers, where N = No. of layers\r\n",
        "        self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout_rate) for _ in range(nb_layers)]\r\n",
        "    \r\n",
        "    def call(self, inputs, mask, training, flag):\r\n",
        "        outputs = self.embedding(inputs)\r\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "        outputs = self.pos_encoding(outputs)\r\n",
        "        outputs = self.dropout(outputs, training)\r\n",
        "        \r\n",
        "        #loop through and call each encoding layer Nb layers of time\r\n",
        "        for i in range(self.nb_layers):\r\n",
        "            outputs = self.enc_layers[i](outputs, mask, training, i, flag)\r\n",
        "\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI5CjemZCrYm",
        "outputId": "835f16c8-2c90-410c-ae8d-47092da92430"
      },
      "source": [
        "enc = Encoder(6, 1024, 8, 0.1, 8192, 512)\r\n",
        "x = tf.random.uniform((64, 20)) #[Batch, Seq]\r\n",
        "enc(x, None, False, False).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 20, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCqKnuUAAkS6"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\r\n",
        "    \r\n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\r\n",
        "        super(DecoderLayer, self).__init__()\r\n",
        "        self.FFN_units = FFN_units\r\n",
        "        self.nb_proj = nb_proj\r\n",
        "        self.dropout_rate = dropout_rate\r\n",
        "    \r\n",
        "    def build(self, input_shape):\r\n",
        "        self.d_model = input_shape[-1]\r\n",
        "        \r\n",
        "        # Define first Multi head attention with itself\r\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\r\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\r\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        \r\n",
        "        # Second Multi head attention combined with encoder output\r\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\r\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\r\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        \r\n",
        "        # Feed foward Network\r\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\r\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\r\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\r\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        \r\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training, index, flag):\r\n",
        "        \r\n",
        "        #apply MHA with padding mask\r\n",
        "        attention, wb1 = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\r\n",
        "        attention = self.dropout_1(attention, training)\r\n",
        "        attention = self.norm_1(attention + inputs)\r\n",
        "        \r\n",
        "        #apply MHA with look ahead mask\r\n",
        "        attention_2, wb2 = self.multi_head_attention_2(attention, enc_outputs, enc_outputs, mask_2)\r\n",
        "        attention_2 = self.dropout_2(attention_2, training)\r\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\r\n",
        "        \r\n",
        "        #apply the final FFN layer\r\n",
        "        outputs = self.dense_1(attention_2)\r\n",
        "        outputs = self.dense_2(outputs)\r\n",
        "        outputs = self.dropout_3(outputs, training)\r\n",
        "        outputs = self.norm_3(outputs + attention_2)\r\n",
        "        \r\n",
        "        return outputs, wb1, wb2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tilHdNz4Eji6"
      },
      "source": [
        "class Decoder(layers.Layer):\r\n",
        "    \r\n",
        "    def __init__(self, nb_layers, FFN_units, nb_proj, dropout_rate, vocab_size, d_model, name=\"decoder\"):\r\n",
        "        super(Decoder, self).__init__(name=name)\r\n",
        "        self.d_model = d_model\r\n",
        "        self.nb_layers = nb_layers\r\n",
        "        \r\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\r\n",
        "        self.pos_encoding = PositionalEncoding()\r\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\r\n",
        "        self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout_rate) for i in range(nb_layers)]\r\n",
        "    \r\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training, flag):\r\n",
        "        #apply the decoder layer architecture in steps\r\n",
        "        outputs = self.embedding(inputs)\r\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n",
        "        outputs = self.pos_encoding(outputs)\r\n",
        "        outputs = self.dropout(outputs, training)\r\n",
        "\r\n",
        "        for i in range(self.nb_layers):\r\n",
        "            #loop through and call all such decoder layer instances\r\n",
        "            outputs, wb1, wb2 = self.dec_layers[i](outputs, enc_outputs, mask_1, mask_2, training, i, flag) \r\n",
        "            #[Batch, Seq, d_model]\r\n",
        "\r\n",
        "        return outputs, wb2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zphVIywFswBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94079db-2045-47a8-c265-d727305d32d0"
      },
      "source": [
        "dec = Decoder(6, 1024, 8, 0.1, 8232, 512)\r\n",
        "x = tf.random.uniform((64, 20)) #[Batch, Seq]\r\n",
        "y = enc(x, None, False, False)\r\n",
        "a, b = dec(x, y, None, None, False, False) #[Batch, Seq, D_model]\r\n",
        "a.shape, b.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 20, 512]), TensorShape([64, 8, 20, 20]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFV25pKmFj9j"
      },
      "source": [
        "###**Now Lets! combine everything together to build our final transformer architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obdURUudF8w9"
      },
      "source": [
        "class Transformer(tf.keras.Model):\r\n",
        "    \r\n",
        "    def __init__(self, vocab_size_enc, vocab_size_dec, d_model, nb_layers, FFN_units, nb_proj, dropout_rate, name=\"transformer\"):\r\n",
        "        super(Transformer, self).__init__(name=name)\r\n",
        "        \r\n",
        "        #Define Encoder, decoder and final linear layer\r\n",
        "        self.encoder = Encoder(nb_layers, FFN_units, nb_proj, dropout_rate, vocab_size_enc, d_model)\r\n",
        "        self.decoder = Decoder(nb_layers, FFN_units, nb_proj, dropout_rate, vocab_size_dec, d_model)\r\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"final_output\")\r\n",
        "    \r\n",
        "    def create_padding_mask(self, seq):\r\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\r\n",
        "        #We add 2 empty dimesions [Batch, nb.proj, seq, seq] since masking is done after scaled dot product \r\n",
        "        #which has input dimension of size = [batch, nb.proj, seq, seq].\r\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\r\n",
        "\r\n",
        "    def create_look_ahead_mask(self, seq):\r\n",
        "        seq_len = tf.shape(seq)[1]\r\n",
        "        #here we only consider the lower left traingle and hide upper right traingle of the matrix\r\n",
        "        #the value -1 means keep the lower left traingle and 0 -> disable upper right traingle\r\n",
        "        #(using the linalg.band_part function).\r\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\r\n",
        "        return look_ahead_mask\r\n",
        "    \r\n",
        "    def call(self, enc_inputs, dec_inputs, training, flag):\r\n",
        "        #encoder mask\r\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\r\n",
        "        #decoder first mask \r\n",
        "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs), self.create_look_ahead_mask(dec_inputs))\r\n",
        "        #decoder second mask, here we use encoder inputs since our keys and values to MHA are from the output of encoder \r\n",
        "        #and queries come from decoder side. We want to mask encoder padded outputs when we recombine with decoder inputs.\r\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\r\n",
        "        \r\n",
        "        #get the encoder outputs\r\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training, flag)\r\n",
        "        #get the decoder outputs\r\n",
        "        dec_outputs, weights = self.decoder(dec_inputs, enc_outputs, dec_mask_1, dec_mask_2, training, flag)\r\n",
        "        \r\n",
        "        #apply the final output layer of unit size = decoder vocab size (such that the model will \r\n",
        "        #predict the words from swedish vocab that have highest probabilities given english input sentence.)\r\n",
        "        outputs = self.last_linear(dec_outputs)\r\n",
        "        \r\n",
        "        return outputs, weights  #[Batch, Seq, Voacab_size_dec]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQv-faWRfJvh",
        "outputId": "10e9d99f-291c-40d9-efb1-8009eeea3e2e"
      },
      "source": [
        "tf.linalg.band_part(tf.ones((10, 10)), 0, -1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 10), dtype=float32, numpy=\n",
              "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLUE83fuddTr"
      },
      "source": [
        "def create_padding_mask(seq):\r\n",
        "  mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\r\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSQx_6Y3eq9N"
      },
      "source": [
        "def create_look_ahead_mask(seq):\r\n",
        "  seq_len = tf.shape(seq)[1]\r\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\r\n",
        "  return look_ahead_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unCAQECYdqL6",
        "outputId": "b372ac88-5077-4f6e-f860-8f216f454b6b"
      },
      "source": [
        "seq = tf.cast([[583, 288, 0, 412, 103, 0, 0, 0]], tf.int32)\r\n",
        "create_padding_mask(seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 1, 8), dtype=float32, numpy=array([[[[0., 0., 1., 0., 0., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRQMdTSYewLq",
        "outputId": "f102ed9c-0fca-4865-cb7f-21b22eca8546"
      },
      "source": [
        "create_look_ahead_mask(seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8, 8), dtype=float32, numpy=\n",
              "array([[0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYSJ_j-ffxq4",
        "outputId": "2c42b1a3-0e5b-4aa6-c3ca-c99cf772562c"
      },
      "source": [
        "#automatically reshapes into [..., seq, seq] and compares with look ahead max\r\n",
        "#this operation helps us apply both the mask!\r\n",
        "tf.maximum(create_padding_mask(seq), create_look_ahead_mask(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 8, 8), dtype=float32, numpy=\n",
              "array([[[[0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrbJu9K1n0pB"
      },
      "source": [
        "tm = Transformer(10000, 10000, 512, 6, 1024, 8, 0.1)\r\n",
        "t_input = tf.random.uniform((1, 20), dtype=tf.int64, minval=0, maxval=200)\r\n",
        "t_target = tf.random.uniform((1, 20), dtype=tf.int64, minval=0, maxval=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wBuoV3Po-rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca935f27-49e4-4bb1-91ca-e12f54b31b71"
      },
      "source": [
        "a, w = tm(t_input, t_target, False, False)\r\n",
        "a.shape, w.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 20, 10000]), TensorShape([1, 8, 20, 20]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNrcHi57XR_J",
        "outputId": "48404cb9-f7c8-4079-c7de-bc6242462992"
      },
      "source": [
        "head = 0\r\n",
        "# shape: (batch=1, num_heads, seq_len_q, seq_len_k)\r\n",
        "attention_heads = tf.squeeze(w, 0)\r\n",
        "attention = attention_heads[head]\r\n",
        "attention.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([20, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crjPBv53feyn"
      },
      "source": [
        "**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYmZN_JhQIln"
      },
      "source": [
        "#tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "# Lets define the Hyper-parameters needed to train our dataset\r\n",
        "# we initially choose small size parameter values for faster training compared to as stated in the paper!\r\n",
        "D_MODEL = 128       # 512\r\n",
        "NB_LAYERS = 4       # 6\r\n",
        "FFN_UNITS = 512     # 2048\r\n",
        "NB_PROJ = 8         # 8\r\n",
        "DROPOUT_RATE = 0.1  # 0.1\r\n",
        "\r\n",
        "#Instantiate the transformer model\r\n",
        "transformer = Transformer(VOCAB_SIZE_EN, VOCAB_SIZE_SV, D_MODEL, NB_LAYERS, FFN_UNITS, NB_PROJ, DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGBDqGP9Roxn"
      },
      "source": [
        "**Now before we start training we need to do few very important steps:**<br><br>1) First we define our loss object as Sparse CategoricalCrossentropy (we use this crossentropy loss function since in the output we have two or more class labels to predict.)<br><br>2) Next we define loss function that creates a mask to hide the padded values and do not include them in the computaion of loss metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpSE_ri8RgxF"
      },
      "source": [
        "#since our outputs from model are real numbers ready to be transformed into probabilities we set from_logits = True.\r\n",
        "#And reduction none indicates dont sum over all probabilities and calc mean loss as of yet. Since we need to remove the \r\n",
        "#padding part before summing the loss!\r\n",
        "\r\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\r\n",
        "\r\n",
        "def loss_function(target, pred):\r\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\r\n",
        "    loss = loss_object(target, pred)\r\n",
        "    \r\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\r\n",
        "    loss *= mask\r\n",
        "    \r\n",
        "    return tf.reduce_mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwv77A7hiwHc"
      },
      "source": [
        "**TODO:<br> \r\n",
        "1) Custom learning rate as descibed in paper<br>\r\n",
        "2) Training the model<br>\r\n",
        "3) Evaluation on test data....** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxNlVu21cM2X"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "train_loss = tf.keras.metrics.Mean(name=\"track_training_loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5LtK3Z_cTLR"
      },
      "source": [
        "#### Test training: to check if loss decresases over time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdlFMNCRcRxG",
        "outputId": "308bc319-4ccf-4f47-9893-433422875645"
      },
      "source": [
        "EPOCHS = 1\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "    print(\"epoch: \", epoch+1)\r\n",
        "    start = time.time()\r\n",
        "    train_loss.reset_states()\r\n",
        "    \r\n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\r\n",
        "        #get everything except last word for input to the decoder\r\n",
        "        dec_inputs = targets[:, :-1]\r\n",
        "\r\n",
        "        #and the output from decoder is the shifted right part.\r\n",
        "        dec_outputs_real = targets[:, 1:]\r\n",
        "\r\n",
        "        #store everything that happens during training on a tape\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            predictions, _ = transformer(enc_inputs, dec_inputs, True, False) #predict\r\n",
        "            loss = loss_function(dec_outputs_real, predictions) #calc loss\r\n",
        "            \r\n",
        "        \r\n",
        "        #Calc gradients dL/dw\r\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\r\n",
        "        #update the weights\r\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\r\n",
        "        \r\n",
        "        train_loss(loss)\r\n",
        "\r\n",
        "        if batch % 50 == 0:\r\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f}\".format(epoch+1, batch, train_loss.result()))\r\n",
        "\r\n",
        "    print(\"Time taken for 1 epoch: \", time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  1\n",
            "Epoch 1 Batch 0 Loss 5.8639\n",
            "Epoch 1 Batch 50 Loss 4.7266\n",
            "Epoch 1 Batch 100 Loss 4.5628\n",
            "Epoch 1 Batch 150 Loss 4.4917\n",
            "Epoch 1 Batch 200 Loss 4.4450\n",
            "Epoch 1 Batch 250 Loss 4.4181\n",
            "Epoch 1 Batch 300 Loss 4.3800\n",
            "Epoch 1 Batch 350 Loss 4.3467\n",
            "Epoch 1 Batch 400 Loss 4.3262\n",
            "Epoch 1 Batch 450 Loss 4.3070\n",
            "Epoch 1 Batch 500 Loss 4.2850\n",
            "Epoch 1 Batch 550 Loss 4.2696\n",
            "Epoch 1 Batch 600 Loss 4.2585\n",
            "Epoch 1 Batch 650 Loss 4.2485\n",
            "Epoch 1 Batch 700 Loss 4.2392\n",
            "Epoch 1 Batch 750 Loss 4.2231\n",
            "Epoch 1 Batch 800 Loss 4.2042\n",
            "Epoch 1 Batch 850 Loss 4.1860\n",
            "Epoch 1 Batch 900 Loss 4.1713\n",
            "Epoch 1 Batch 950 Loss 4.1580\n",
            "Epoch 1 Batch 1000 Loss 4.1460\n",
            "Epoch 1 Batch 1050 Loss 4.1366\n",
            "Epoch 1 Batch 1100 Loss 4.1239\n",
            "Epoch 1 Batch 1150 Loss 4.1149\n",
            "Epoch 1 Batch 1200 Loss 4.1033\n",
            "Epoch 1 Batch 1250 Loss 4.0949\n",
            "Epoch 1 Batch 1300 Loss 4.0859\n",
            "Epoch 1 Batch 1350 Loss 4.0827\n",
            "Epoch 1 Batch 1400 Loss 4.0800\n",
            "Epoch 1 Batch 1450 Loss 4.0728\n",
            "Epoch 1 Batch 1500 Loss 4.0691\n",
            "Epoch 1 Batch 1550 Loss 4.0660\n",
            "Epoch 1 Batch 1600 Loss 4.0617\n",
            "Epoch 1 Batch 1650 Loss 4.0601\n",
            "Epoch 1 Batch 1700 Loss 4.0570\n",
            "Epoch 1 Batch 1750 Loss 4.0553\n",
            "Epoch 1 Batch 1800 Loss 4.0520\n",
            "Epoch 1 Batch 1850 Loss 4.0501\n",
            "Epoch 1 Batch 1900 Loss 4.0467\n",
            "Epoch 1 Batch 1950 Loss 4.0425\n",
            "Epoch 1 Batch 2000 Loss 4.0383\n",
            "Epoch 1 Batch 2050 Loss 4.0357\n",
            "Epoch 1 Batch 2100 Loss 4.0325\n",
            "Epoch 1 Batch 2150 Loss 4.0284\n",
            "Epoch 1 Batch 2200 Loss 4.0260\n",
            "Epoch 1 Batch 2250 Loss 4.0227\n",
            "Epoch 1 Batch 2300 Loss 4.0172\n",
            "Epoch 1 Batch 2350 Loss 4.0135\n",
            "Epoch 1 Batch 2400 Loss 4.0093\n",
            "Epoch 1 Batch 2450 Loss 4.0061\n",
            "Epoch 1 Batch 2500 Loss 4.0016\n",
            "Epoch 1 Batch 2550 Loss 3.9977\n",
            "Epoch 1 Batch 2600 Loss 3.9943\n",
            "Epoch 1 Batch 2650 Loss 3.9915\n",
            "Epoch 1 Batch 2700 Loss 3.9877\n",
            "Epoch 1 Batch 2750 Loss 3.9844\n",
            "Epoch 1 Batch 2800 Loss 3.9824\n",
            "Epoch 1 Batch 2850 Loss 3.9805\n",
            "Epoch 1 Batch 2900 Loss 3.9787\n",
            "Epoch 1 Batch 2950 Loss 3.9780\n",
            "Epoch 1 Batch 3000 Loss 3.9777\n",
            "Epoch 1 Batch 3050 Loss 3.9777\n",
            "Epoch 1 Batch 3100 Loss 3.9779\n",
            "Epoch 1 Batch 3150 Loss 3.9779\n",
            "Epoch 1 Batch 3200 Loss 3.9778\n",
            "Epoch 1 Batch 3250 Loss 3.9783\n",
            "Epoch 1 Batch 3300 Loss 3.9779\n",
            "Epoch 1 Batch 3350 Loss 3.9787\n",
            "Epoch 1 Batch 3400 Loss 3.9793\n",
            "Epoch 1 Batch 3450 Loss 3.9799\n",
            "Epoch 1 Batch 3500 Loss 3.9800\n",
            "Epoch 1 Batch 3550 Loss 3.9798\n",
            "Epoch 1 Batch 3600 Loss 3.9810\n",
            "Epoch 1 Batch 3650 Loss 3.9816\n",
            "Epoch 1 Batch 3700 Loss 3.9821\n",
            "Epoch 1 Batch 3750 Loss 3.9826\n",
            "Epoch 1 Batch 3800 Loss 3.9830\n",
            "Epoch 1 Batch 3850 Loss 3.9824\n",
            "Epoch 1 Batch 3900 Loss 3.9829\n",
            "Epoch 1 Batch 3950 Loss 3.9837\n",
            "Epoch 1 Batch 4000 Loss 3.9839\n",
            "Epoch 1 Batch 4050 Loss 3.9841\n",
            "Epoch 1 Batch 4100 Loss 3.9837\n",
            "Epoch 1 Batch 4150 Loss 3.9836\n",
            "Epoch 1 Batch 4200 Loss 3.9829\n",
            "Epoch 1 Batch 4250 Loss 3.9826\n",
            "Epoch 1 Batch 4300 Loss 3.9825\n",
            "Epoch 1 Batch 4350 Loss 3.9822\n",
            "Epoch 1 Batch 4400 Loss 3.9821\n",
            "Epoch 1 Batch 4450 Loss 3.9818\n",
            "Epoch 1 Batch 4500 Loss 3.9815\n",
            "Epoch 1 Batch 4550 Loss 3.9812\n",
            "Epoch 1 Batch 4600 Loss 3.9809\n",
            "Epoch 1 Batch 4650 Loss 3.9813\n",
            "Epoch 1 Batch 4700 Loss 3.9813\n",
            "Epoch 1 Batch 4750 Loss 3.9810\n",
            "Epoch 1 Batch 4800 Loss 3.9810\n",
            "Epoch 1 Batch 4850 Loss 3.9816\n",
            "Epoch 1 Batch 4900 Loss 3.9817\n",
            "Epoch 1 Batch 4950 Loss 3.9819\n",
            "Epoch 1 Batch 5000 Loss 3.9821\n",
            "Epoch 1 Batch 5050 Loss 3.9817\n",
            "Epoch 1 Batch 5100 Loss 3.9821\n",
            "Epoch 1 Batch 5150 Loss 3.9828\n",
            "Epoch 1 Batch 5200 Loss 3.9830\n",
            "Epoch 1 Batch 5250 Loss 3.9829\n",
            "Epoch 1 Batch 5300 Loss 3.9833\n",
            "Epoch 1 Batch 5350 Loss 3.9835\n",
            "Epoch 1 Batch 5400 Loss 3.9840\n",
            "Epoch 1 Batch 5450 Loss 3.9845\n",
            "Epoch 1 Batch 5500 Loss 3.9849\n",
            "Epoch 1 Batch 5550 Loss 3.9856\n",
            "Epoch 1 Batch 5600 Loss 3.9860\n",
            "Epoch 1 Batch 5650 Loss 3.9861\n",
            "Epoch 1 Batch 5700 Loss 3.9864\n",
            "Epoch 1 Batch 5750 Loss 3.9868\n",
            "Epoch 1 Batch 5800 Loss 3.9871\n",
            "Epoch 1 Batch 5850 Loss 3.9876\n",
            "Epoch 1 Batch 5900 Loss 3.9879\n",
            "Epoch 1 Batch 5950 Loss 3.9882\n",
            "Epoch 1 Batch 6000 Loss 3.9884\n",
            "Epoch 1 Batch 6050 Loss 3.9885\n",
            "Epoch 1 Batch 6100 Loss 3.9889\n",
            "Epoch 1 Batch 6150 Loss 3.9888\n",
            "Epoch 1 Batch 6200 Loss 3.9891\n",
            "Epoch 1 Batch 6250 Loss 3.9890\n",
            "Epoch 1 Batch 6300 Loss 3.9891\n",
            "Epoch 1 Batch 6350 Loss 3.9890\n",
            "Epoch 1 Batch 6400 Loss 3.9887\n",
            "Epoch 1 Batch 6450 Loss 3.9884\n",
            "Epoch 1 Batch 6500 Loss 3.9874\n",
            "Epoch 1 Batch 6550 Loss 3.9869\n",
            "Epoch 1 Batch 6600 Loss 3.9866\n",
            "Epoch 1 Batch 6650 Loss 3.9858\n",
            "Epoch 1 Batch 6700 Loss 3.9849\n",
            "Epoch 1 Batch 6750 Loss 3.9844\n",
            "Epoch 1 Batch 6800 Loss 3.9839\n",
            "Epoch 1 Batch 6850 Loss 3.9833\n",
            "Epoch 1 Batch 6900 Loss 3.9826\n",
            "Time taken for 1 epoch:  1815.225375175476\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}